{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNJAvAxjkBEcdvUwSyWRYxs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00933db3fb2d4493a4efa99f5d6503d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f0da2a4d9fc74529a9cf4b390df540a7",
              "IPY_MODEL_128eac2310914a2c8060ffac599844c2",
              "IPY_MODEL_d0d2eda6be4c41da8bee943eeabcb714"
            ],
            "layout": "IPY_MODEL_5711e1e6b41f48639d765821951c7b0e"
          }
        },
        "f0da2a4d9fc74529a9cf4b390df540a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6efa41aaaef4b20896b832e84990649",
            "placeholder": "​",
            "style": "IPY_MODEL_6c024bc4381c4631979c8d2e98ba1fe5",
            "value": "Generating train split: "
          }
        },
        "128eac2310914a2c8060ffac599844c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_716b714c32424e19abea4c6a2316c128",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d1f6f4eb973a4e3b84e30dee2f9fe73e",
            "value": 1
          }
        },
        "d0d2eda6be4c41da8bee943eeabcb714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27cfb359b5994357a7060859b2534f04",
            "placeholder": "​",
            "style": "IPY_MODEL_1b8f1be082ae4c659272e8365bc151a4",
            "value": " 2000/0 [00:00&lt;00:00, 98333.19 examples/s]"
          }
        },
        "5711e1e6b41f48639d765821951c7b0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6efa41aaaef4b20896b832e84990649": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c024bc4381c4631979c8d2e98ba1fe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "716b714c32424e19abea4c6a2316c128": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "d1f6f4eb973a4e3b84e30dee2f9fe73e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27cfb359b5994357a7060859b2534f04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b8f1be082ae4c659272e8365bc151a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cacdb7c813db43baa29786a8baff2069": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed921cb082c643dcb9e94314664b0b69",
              "IPY_MODEL_6a029b89a6c547a19ed9610c4ae6f120",
              "IPY_MODEL_86882712d8b548d6b6771f0791f97833"
            ],
            "layout": "IPY_MODEL_24ed353fd09b4341927e41722bcf5373"
          }
        },
        "ed921cb082c643dcb9e94314664b0b69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd3dfbc44dbf4746b3029876b3a62f46",
            "placeholder": "​",
            "style": "IPY_MODEL_9b76c4fa8b7445638b5c49f0929d43cf",
            "value": "Tokenizing dataset: 100%"
          }
        },
        "6a029b89a6c547a19ed9610c4ae6f120": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_158320c9e75a40eeafc0481529d10c2c",
            "max": 2000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6580583a99d346de9608768c1e57aa94",
            "value": 2000
          }
        },
        "86882712d8b548d6b6771f0791f97833": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45228e3b2b71430e986c3ec650dd5f22",
            "placeholder": "​",
            "style": "IPY_MODEL_7256fc7ae1fe4777bb1ae974a3fb179e",
            "value": " 2000/2000 [00:01&lt;00:00, 1312.01 examples/s]"
          }
        },
        "24ed353fd09b4341927e41722bcf5373": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd3dfbc44dbf4746b3029876b3a62f46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b76c4fa8b7445638b5c49f0929d43cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "158320c9e75a40eeafc0481529d10c2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6580583a99d346de9608768c1e57aa94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "45228e3b2b71430e986c3ec650dd5f22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7256fc7ae1fe4777bb1ae974a3fb179e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohit01-zoey/gemma270m-competition/blob/main/arcc-lora/lora_for_finetuned_arc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4A3YgsSfe9Fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0ec2556-c7d3-4cc9-eb10-e3da012ee515"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers tokenizers peft -y\n",
        "!pip install --upgrade transformers accelerate peft datasets bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5VrUh3a-kCm",
        "outputId": "45bc0313-b80a-4a92-dbbb-d98551c4fa3a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.57.3\n",
            "Uninstalling transformers-4.57.3:\n",
            "  Successfully uninstalled transformers-4.57.3\n",
            "Found existing installation: tokenizers 0.22.1\n",
            "Uninstalling tokenizers-0.22.1:\n",
            "  Successfully uninstalled tokenizers-0.22.1\n",
            "Found existing installation: peft 0.18.0\n",
            "Uninstalling peft-0.18.0:\n",
            "  Successfully uninstalled peft-0.18.0\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Collecting peft\n",
            "  Using cached peft-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
            "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Using cached transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
            "Using cached peft-0.18.0-py3-none-any.whl (556 kB)\n",
            "Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "Installing collected packages: tokenizers, transformers, peft\n",
            "Successfully installed peft-0.18.0 tokenizers-0.22.1 transformers-4.57.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import json\n",
        "\n",
        "# Output file\n",
        "out_path = \"/content/arc_easy_sft.jsonl\"\n",
        "\n",
        "# Load ARC-Easy instead of ARC-Challenge\n",
        "arc_easy = load_dataset(\"allenai/ai2_arc\", \"ARC-Easy\")\n",
        "\n",
        "# Helper: convert \"choices\" dict into letter→text mapping\n",
        "def choices_to_string(choices):\n",
        "    out = []\n",
        "    for c in choices[\"label\"]:\n",
        "        idx = choices[\"label\"].index(c)\n",
        "        txt = choices[\"text\"][idx]\n",
        "        out.append(f\"({c}) {txt}\")\n",
        "    return \"\\n\".join(out)\n",
        "\n",
        "# Build SFT lines\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as fout:\n",
        "    for item in arc_easy[\"train\"]:\n",
        "        q = item[\"question\"].strip()\n",
        "        choices = item[\"choices\"]\n",
        "        answer = item[\"answerKey\"].strip()\n",
        "\n",
        "        # Format prompt\n",
        "        prompt = q + \"\\nOptions:\\n\" + choices_to_string(choices)\n",
        "\n",
        "        record = {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "                {\"role\": \"assistant\", \"content\": answer}\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        fout.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"✓ Wrote ARC-Easy dataset to: {out_path}\")\n",
        "print(f\"Total examples: {len(arc_easy['train'])}\")\n",
        "\n",
        "# Update the data path\n",
        "SFT_DATA_PATH = \"/content/arc_easy_sft.jsonl\""
      ],
      "metadata": {
        "id": "ZGMTbA8Jpykm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ec2abeb-c9db-4727-f5d5-fa4a57efff78"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Wrote ARC-Easy dataset to: /content/arc_easy_sft.jsonl\n",
            "Total examples: 2251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import json\n",
        "\n",
        "out_path = \"/content/boolq_sft.jsonl\"\n",
        "\n",
        "# Load BoolQ - yes/no questions (very different format)\n",
        "boolq = load_dataset(\"google/boolq\", split=\"train\")\n",
        "\n",
        "# Take a subset\n",
        "boolq = boolq.shuffle(seed=42).select(range(2000))\n",
        "\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as fout:\n",
        "    for item in boolq:\n",
        "        question = item[\"question\"].strip()\n",
        "        passage = item[\"passage\"].strip()\n",
        "        answer = \"Yes\" if item[\"answer\"] else \"No\"\n",
        "\n",
        "        # Format: passage + question\n",
        "        prompt = f\"Passage: {passage}\\n\\nQuestion: {question}\\nAnswer with Yes or No.\"\n",
        "\n",
        "        record = {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "                {\"role\": \"assistant\", \"content\": answer}\n",
        "            ]\n",
        "        }\n",
        "        fout.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"✓ Wrote BoolQ dataset: {out_path}\")\n",
        "\n",
        "# Update paths\n",
        "SFT_DATA_PATH = \"/content/boolq_sft.jsonl\"\n",
        "#LORA_OUTPUT_DIR = \"/content/drive/MyDrive/gemma3_lora_boolq\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64MGSuXmSHPt",
        "outputId": "179c1e7a-27db-4948-989e-a2bcac08d9b4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Wrote BoolQ dataset: /content/boolq_sft.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IFT_CHECKPOINT = \"/content/drive/MyDrive/checkpoint-85000-darsh\"\n",
        "LORA_OUTPUT_DIR = \"/content/drive/MyDrive/gemma3_lora_sft_for_IFT_ARC-C\"\n",
        "EPOCHS = 3"
      ],
      "metadata": {
        "id": "g0CW2N8JfIa7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qBZEtc_IpvJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "import os\n",
        "import json\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Fix tokenizer_config.json - REMOVE the problematic fields\n",
        "tokenizer_config_path = os.path.join(IFT_CHECKPOINT, \"tokenizer_config.json\")\n",
        "\n",
        "# Read the config\n",
        "with open(tokenizer_config_path, 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Remove ALL potentially problematic fields\n",
        "problematic_fields = [\"model_specific_special_tokens\", \"extra_special_tokens\"]\n",
        "for field in problematic_fields:\n",
        "    if field in config:\n",
        "        print(f\"✓ Removing {field} field\")\n",
        "        del config[field]\n",
        "\n",
        "# Save the fixed config\n",
        "with open(tokenizer_config_path, 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(\"✓ Fixed tokenizer_config.json\")\n",
        "\n",
        "# Clear any cached tokenizer\n",
        "import importlib\n",
        "import transformers\n",
        "if hasattr(transformers, 'tokenization_utils_base'):\n",
        "    importlib.reload(transformers.tokenization_utils_base)\n",
        "\n",
        "# Load tokenizer\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    IFT_CHECKPOINT,\n",
        "    trust_remote_code=True,\n",
        "    use_fast=True,\n",
        "    local_files_only=True\n",
        ")\n",
        "\n",
        "# Ensure padding token is set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"✓ Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
        "\n",
        "print(\"Loading model...\")\n",
        "# Load config first\n",
        "config = AutoConfig.from_pretrained(\n",
        "    IFT_CHECKPOINT,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    IFT_CHECKPOINT,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float32,  # Use full precision\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"✓ Model loaded successfully\")"
      ],
      "metadata": {
        "id": "8NmlccyvfM9O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43bff1b2-8fef-41e4-f16b-b3b09987c118"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "✓ Fixed tokenizer_config.json\n",
            "Loading tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer you are loading from '/content/drive/MyDrive/checkpoint-85000-darsh' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Tokenizer loaded: GemmaTokenizerFast\n",
            "Loading model...\n",
            "✓ Model loaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Configuring LoRA...\")\n",
        "\n",
        "# For Gemma3, target the attention projection layers\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "# Apply LoRA to model\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "CZNSy1FifQMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b65bfa3-2486-42da-e0f8-f12a77038832"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuring LoRA...\n",
            "trainable params: 1,474,560 || all params: 269,574,656 || trainable%: 0.5470\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "print(\"Re-tokenizing WITHOUT aggressive masking...\")\n",
        "\n",
        "raw_dataset = load_dataset(\"json\", data_files={\"train\": SFT_DATA_PATH})\n",
        "dataset = raw_dataset[\"train\"]\n",
        "\n",
        "def format_and_tokenize(example):\n",
        "    # Apply chat template\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        example[\"messages\"],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "\n",
        "    # Tokenize the full text\n",
        "    tokenized = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=False,\n",
        "        return_tensors=None,\n",
        "        add_special_tokens=True\n",
        "    )\n",
        "\n",
        "    # DON'T mask anything - train on full sequence\n",
        "    # This is appropriate for your task\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    format_and_tokenize,\n",
        "    remove_columns=dataset.column_names,\n",
        "    desc=\"Tokenizing dataset\"\n",
        ")\n",
        "\n",
        "print(f\"✓ Dataset tokenized: {len(tokenized_dataset)} examples\")\n",
        "\n",
        "# Verify\n",
        "sample = tokenized_dataset[0]\n",
        "masked = sum(1 for x in sample['labels'] if x == -100)\n",
        "valid = sum(1 for x in sample['labels'] if x != -100)\n",
        "print(f\"  Masked tokens: {masked}\")\n",
        "print(f\"  Valid tokens: {valid}\")\n",
        "print(f\"  Masking ratio: {masked / len(sample['labels']) * 100:.1f}%\")"
      ],
      "metadata": {
        "id": "ku_bCYghfSvw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168,
          "referenced_widgets": [
            "00933db3fb2d4493a4efa99f5d6503d2",
            "f0da2a4d9fc74529a9cf4b390df540a7",
            "128eac2310914a2c8060ffac599844c2",
            "d0d2eda6be4c41da8bee943eeabcb714",
            "5711e1e6b41f48639d765821951c7b0e",
            "a6efa41aaaef4b20896b832e84990649",
            "6c024bc4381c4631979c8d2e98ba1fe5",
            "716b714c32424e19abea4c6a2316c128",
            "d1f6f4eb973a4e3b84e30dee2f9fe73e",
            "27cfb359b5994357a7060859b2534f04",
            "1b8f1be082ae4c659272e8365bc151a4",
            "cacdb7c813db43baa29786a8baff2069",
            "ed921cb082c643dcb9e94314664b0b69",
            "6a029b89a6c547a19ed9610c4ae6f120",
            "86882712d8b548d6b6771f0791f97833",
            "24ed353fd09b4341927e41722bcf5373",
            "dd3dfbc44dbf4746b3029876b3a62f46",
            "9b76c4fa8b7445638b5c49f0929d43cf",
            "158320c9e75a40eeafc0481529d10c2c",
            "6580583a99d346de9608768c1e57aa94",
            "45228e3b2b71430e986c3ec650dd5f22",
            "7256fc7ae1fe4777bb1ae974a3fb179e"
          ]
        },
        "outputId": "64ef6532-3e80-4677-c7cd-4476ec115f52"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Re-tokenizing WITHOUT aggressive masking...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00933db3fb2d4493a4efa99f5d6503d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cacdb7c813db43baa29786a8baff2069"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Dataset tokenized: 2000 examples\n",
            "  Masked tokens: 0\n",
            "  Valid tokens: 153\n",
            "  Masking ratio: 0.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detailed diagnostic\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TOKENIZATION DIAGNOSTIC\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "sample_idx = 0\n",
        "sample = tokenized_dataset[sample_idx]\n",
        "\n",
        "print(f\"\\n1. Lengths:\")\n",
        "print(f\"   Input IDs: {len(sample['input_ids'])}\")\n",
        "print(f\"   Labels: {len(sample['labels'])}\")\n",
        "\n",
        "print(f\"\\n2. Label masking:\")\n",
        "masked_count = sum(1 for x in sample['labels'] if x == -100)\n",
        "actual_count = sum(1 for x in sample['labels'] if x != -100)\n",
        "print(f\"   Masked tokens (-100): {masked_count}\")\n",
        "print(f\"   Actual labels: {actual_count}\")\n",
        "print(f\"   Masking ratio: {masked_count / len(sample['labels']):.2%}\")\n",
        "\n",
        "print(f\"\\n3. Decoded sample:\")\n",
        "print(\"   Full input:\")\n",
        "full_text = tokenizer.decode(sample['input_ids'])\n",
        "print(f\"   {full_text[:300]}...\")\n",
        "\n",
        "print(\"\\n   Labels only (non-masked):\")\n",
        "label_tokens = [tok if tok != -100 else tokenizer.pad_token_id for tok in sample['labels']]\n",
        "actual_labels = [tok for tok in sample['labels'] if tok != -100]\n",
        "if actual_labels:\n",
        "    label_text = tokenizer.decode(actual_labels)\n",
        "    print(f\"   '{label_text}'\")\n",
        "    print(f\"   Raw token IDs: {actual_labels}\")\n",
        "\n",
        "print(\"\\n4. First 30 tokens comparison:\")\n",
        "for i in range(min(30, len(sample['input_ids']))):\n",
        "    token = tokenizer.decode([sample['input_ids'][i]])\n",
        "    label = sample['labels'][i]\n",
        "    print(f\"   {i:3d}: '{token:15s}' | Label: {label if label != -100 else 'MASKED'}\")\n",
        "\n",
        "print(\"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUaJcWKtOtVc",
        "outputId": "3ea20300-de46-4304-f1ce-17aef8b79c99"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "TOKENIZATION DIAGNOSTIC\n",
            "==================================================\n",
            "\n",
            "1. Lengths:\n",
            "   Input IDs: 153\n",
            "   Labels: 153\n",
            "\n",
            "2. Label masking:\n",
            "   Masked tokens (-100): 0\n",
            "   Actual labels: 153\n",
            "   Masking ratio: 0.00%\n",
            "\n",
            "3. Decoded sample:\n",
            "   Full input:\n",
            "   <|im_start|><|im_start|>user\n",
            "Passage: Henry Daniel Mills is a fictional character in ABC's television series Once Upon a Time. Henry is the boy Emma Swan gave up to adoption; Regina Mills adopted him. Henry was originally portrayed as a child by Jared S. Gilmore, who won the Young Artist Award for B...\n",
            "\n",
            "   Labels only (non-masked):\n",
            "   '<|im_start|><|im_start|>user\n",
            "Passage: Henry Daniel Mills is a fictional character in ABC's television series Once Upon a Time. Henry is the boy Emma Swan gave up to adoption; Regina Mills adopted him. Henry was originally portrayed as a child by Jared S. Gilmore, who won the Young Artist Award for Best Performance in a TV Series -- Leading Young Actor in 2012. For the show's seventh and final season, Andrew J. West later took over the role of Henry as an adult and father to a eight-year-old girl named Lucy, with Gilmore also making three appearances as Henry during the season.\n",
            "\n",
            "Question: did henry die in once upon a time\n",
            "Answer with Yes or No.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "No<|im_end|>\n",
            "'\n",
            "   Raw token IDs: [262145, 262145, 2364, 107, 8653, 676, 236787, 12297, 13108, 40161, 563, 496, 57728, 2872, 528, 21593, 236789, 236751, 13617, 3605, 9920, 26831, 496, 7578, 236761, 12297, 563, 506, 6938, 36569, 51166, 5877, 872, 531, 22109, 236793, 71227, 40161, 12569, 1515, 236761, 12297, 691, 13320, 54056, 618, 496, 1919, 684, 79013, 555, 236761, 143387, 236764, 1015, 2810, 506, 11853, 40949, 13835, 573, 8890, 19791, 528, 496, 6115, 12142, 2617, 74924, 11853, 58469, 528, 236743, 236778, 236771, 236770, 236778, 236761, 1701, 506, 1407, 236789, 236751, 29210, 532, 1626, 3409, 236764, 15584, 730, 236761, 4844, 3209, 3721, 1024, 506, 3853, 529, 12297, 618, 614, 6779, 532, 6353, 531, 496, 6589, 236772, 3704, 236772, 947, 3953, 7489, 43108, 236764, 607, 143387, 992, 3043, 1806, 33611, 618, 12297, 1913, 506, 3409, 236761, 108, 14977, 236787, 1602, 180939, 1778, 528, 3622, 3324, 496, 990, 107, 7925, 607, 8438, 653, 2301, 236761, 262146, 107, 262145, 111457, 107, 3771, 262146, 107]\n",
            "\n",
            "4. First 30 tokens comparison:\n",
            "     0: '<|im_start|>   ' | Label: 262145\n",
            "     1: '<|im_start|>   ' | Label: 262145\n",
            "     2: 'user           ' | Label: 2364\n",
            "     3: '\n",
            "              ' | Label: 107\n",
            "     4: 'Pass           ' | Label: 8653\n",
            "     5: 'age            ' | Label: 676\n",
            "     6: ':              ' | Label: 236787\n",
            "     7: ' Henry         ' | Label: 12297\n",
            "     8: ' Daniel        ' | Label: 13108\n",
            "     9: ' Mills         ' | Label: 40161\n",
            "    10: ' is            ' | Label: 563\n",
            "    11: ' a             ' | Label: 496\n",
            "    12: ' fictional     ' | Label: 57728\n",
            "    13: ' character     ' | Label: 2872\n",
            "    14: ' in            ' | Label: 528\n",
            "    15: ' ABC           ' | Label: 21593\n",
            "    16: ''              ' | Label: 236789\n",
            "    17: 's              ' | Label: 236751\n",
            "    18: ' television    ' | Label: 13617\n",
            "    19: ' series        ' | Label: 3605\n",
            "    20: ' Once          ' | Label: 9920\n",
            "    21: ' Upon          ' | Label: 26831\n",
            "    22: ' a             ' | Label: 496\n",
            "    23: ' Time          ' | Label: 7578\n",
            "    24: '.              ' | Label: 236761\n",
            "    25: ' Henry         ' | Label: 12297\n",
            "    26: ' is            ' | Label: 563\n",
            "    27: ' the           ' | Label: 506\n",
            "    28: ' boy           ' | Label: 6938\n",
            "    29: ' Emma          ' | Label: 36569\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List\n",
        "import torch\n",
        "\n",
        "# Custom data collator\n",
        "@dataclass\n",
        "class CustomDataCollator:\n",
        "    tokenizer: AutoTokenizer\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "        # Extract input_ids and labels\n",
        "        input_ids = [f[\"input_ids\"] for f in features]\n",
        "        labels = [f[\"labels\"] for f in features]\n",
        "\n",
        "        # Find max length in batch\n",
        "        max_length = max(len(ids) for ids in input_ids)\n",
        "\n",
        "        # Pad sequences\n",
        "        padded_input_ids = []\n",
        "        padded_labels = []\n",
        "        attention_mask = []\n",
        "\n",
        "        for ids, lbls in zip(input_ids, labels):\n",
        "            padding_length = max_length - len(ids)\n",
        "\n",
        "            # Pad input_ids and attention_mask\n",
        "            padded_input_ids.append(ids + [self.tokenizer.pad_token_id] * padding_length)\n",
        "            attention_mask.append([1] * len(ids) + [0] * padding_length)\n",
        "\n",
        "            # Pad labels (use -100 for padding tokens so they're ignored in loss)\n",
        "            padded_labels.append(lbls + [-100] * padding_length)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(padded_input_ids, dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
        "            \"labels\": torch.tensor(padded_labels, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create data collator\n",
        "data_collator = CustomDataCollator(tokenizer=tokenizer)\n",
        "\n",
        "print(\"✓ Custom data collator created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtfN6bpOCC5j",
        "outputId": "4a6ce466-434a-45f5-d08f-b4f56fe98438"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Custom data collator created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual training step to see what's going on\n",
        "print(\"=== MANUAL TRAINING STEP TEST ===\\n\")\n",
        "\n",
        "model.train()\n",
        "\n",
        "# Get a batch\n",
        "batch = data_collator([tokenized_dataset[0], tokenized_dataset[1], tokenized_dataset[2], tokenized_dataset[3]])\n",
        "batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "print(\"1. Batch info:\")\n",
        "print(f\"   Batch size: {batch['input_ids'].shape}\")\n",
        "print(f\"   Device: {batch['input_ids'].device}\")\n",
        "\n",
        "print(\"\\n2. Forward pass:\")\n",
        "outputs = model(**batch)\n",
        "loss = outputs.loss\n",
        "\n",
        "print(f\"   Loss: {loss.item()}\")\n",
        "print(f\"   Loss dtype: {loss.dtype}\")\n",
        "print(f\"   Loss requires_grad: {loss.requires_grad}\")\n",
        "\n",
        "if loss.item() == 0.0:\n",
        "    print(\"\\n   ⚠️ Loss is exactly 0.0 - checking logits...\")\n",
        "    logits = outputs.logits\n",
        "    print(f\"   Logits shape: {logits.shape}\")\n",
        "    print(f\"   Logits range: [{logits.min().item():.4f}, {logits.max().item():.4f}]\")\n",
        "    print(f\"   Logits mean: {logits.mean().item():.4f}\")\n",
        "    print(f\"   Logits std: {logits.std().item():.4f}\")\n",
        "\n",
        "    # Check predictions\n",
        "    predictions = logits.argmax(dim=-1)\n",
        "    print(f\"\\n   Predictions shape: {predictions.shape}\")\n",
        "    print(f\"   First 20 predictions: {predictions[0, :20].tolist()}\")\n",
        "    print(f\"   First 20 labels: {batch['labels'][0, :20].tolist()}\")\n",
        "\n",
        "    # Check accuracy\n",
        "    matches = (predictions == batch['labels']).float()\n",
        "    valid_mask = (batch['labels'] != -100)\n",
        "    accuracy = matches[valid_mask].mean().item()\n",
        "    print(f\"\\n   Accuracy on this batch: {accuracy:.2%}\")\n",
        "\n",
        "print(\"\\n3. Backward pass:\")\n",
        "try:\n",
        "    loss.backward()\n",
        "    print(\"   ✓ Backward pass successful\")\n",
        "\n",
        "    # Check if gradients exist\n",
        "    has_grads = False\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad and param.grad is not None:\n",
        "            has_grads = True\n",
        "            print(f\"   ✓ Gradient found for: {name[:50]}... | Grad mean: {param.grad.mean().item():.6f}\")\n",
        "            break\n",
        "\n",
        "    if not has_grads:\n",
        "        print(\"   ✗ NO GRADIENTS FOUND!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"   ✗ Backward pass failed: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZGm3xozXgQz",
        "outputId": "8186faab-65d5-4693-fb4b-ffcd8012f4ac"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== MANUAL TRAINING STEP TEST ===\n",
            "\n",
            "1. Batch info:\n",
            "   Batch size: torch.Size([4, 218])\n",
            "   Device: cuda:0\n",
            "\n",
            "2. Forward pass:\n",
            "   Loss: 3.488351583480835\n",
            "   Loss dtype: torch.float32\n",
            "   Loss requires_grad: True\n",
            "\n",
            "3. Backward pass:\n",
            "   ✓ Backward pass successful\n",
            "   ✓ Gradient found for: base_model.model.model.layers.0.self_attn.q_proj.l... | Grad mean: 0.000000\n",
            "\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recreate trainer with fresh optimizer state\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=LORA_OUTPUT_DIR,\n",
        "    per_device_train_batch_size=1,  # Smaller batch for stability\n",
        "    gradient_accumulation_steps=16,  # Compensate with more accumulation\n",
        "    learning_rate=1e-4,  # Lower LR for stability\n",
        "    num_train_epochs=EPOCHS,\n",
        "    fp16=False,  # NO FP16 - use full precision\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",\n",
        "    optim=\"adamw_torch\",\n",
        "    warmup_steps=100,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    max_grad_norm=1.0  # Gradient clipping\n",
        ")\n",
        "\n",
        "# Recreate trainer (this resets optimizer)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "print(\"✓ Trainer recreated with fresh optimizer\")\n",
        "\n",
        "# Train\n",
        "print(\"\\nStarting training with full label supervision...\")\n",
        "trainer.train()\n",
        "\n",
        "# Save\n",
        "trainer.save_model(LORA_OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(LORA_OUTPUT_DIR)\n",
        "print(f\"✓ Training complete! LoRA adapters saved to: {LORA_OUTPUT_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NhG1umviXcD3",
        "outputId": "d8eeb033-6b71-49ce-c3fc-01974bd41dab"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2155069652.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Trainer recreated with fresh optimizer\n",
            "\n",
            "Starting training with full label supervision...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [375/375 13:38, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.096200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.069700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>3.022900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.854700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.745500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.760600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.657000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>2.670700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>2.618700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.499700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>2.515400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>2.624000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>2.505700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>2.518000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.433000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>2.417100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>2.575000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>2.531300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>2.552800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.517700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>2.539000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>2.503600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>2.484800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>2.402500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.551000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>2.483600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>2.480200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>2.501400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>2.471400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.458600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>2.493100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>2.449000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>2.513300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>2.450300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>2.521300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>2.412700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>2.507700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Training complete! LoRA adapters saved to: /content/drive/MyDrive/gemma3_lora_sft_for_IFT_ARC-C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Debug: Check if loss is actually being computed\n",
        "print(\"=== DEBUGGING TRAINING ===\")\n",
        "\n",
        "# 1. Check a single batch manually\n",
        "sample_batch = data_collator([tokenized_dataset[0], tokenized_dataset[1]])\n",
        "\n",
        "print(\"\\n1. Sample batch shapes:\")\n",
        "print(f\"   input_ids: {sample_batch['input_ids'].shape}\")\n",
        "print(f\"   labels: {sample_batch['labels'].shape}\")\n",
        "print(f\"   attention_mask: {sample_batch['attention_mask'].shape}\")\n",
        "\n",
        "# 2. Check label content\n",
        "print(\"\\n2. Labels content:\")\n",
        "print(f\"   First 30 labels: {sample_batch['labels'][0][:30]}\")\n",
        "print(f\"   Num valid labels (not -100): {(sample_batch['labels'] != -100).sum()}\")\n",
        "print(f\"   Num -100 labels: {(sample_batch['labels'] == -100).sum()}\")\n",
        "\n",
        "# 3. Manual forward pass\n",
        "print(\"\\n3. Testing manual forward pass:\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Move batch to device\n",
        "    batch_device = {k: v.to(device) for k, v in sample_batch.items()}\n",
        "    outputs = model(**batch_device)\n",
        "    loss = outputs.loss\n",
        "    print(f\"   Manual loss: {loss.item()}\")\n",
        "\n",
        "# 4. Check if model parameters are actually being updated\n",
        "print(\"\\n4. Checking if LoRA params are trainable:\")\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# 5. Check optimizer state\n",
        "if hasattr(trainer, 'optimizer'):\n",
        "    print(\"\\n5. Optimizer learning rate:\")\n",
        "    print(f\"   LR: {trainer.optimizer.param_groups[0]['lr']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVy9xBhjUtkN",
        "outputId": "d8f162da-2aa5-4646-f36c-a821a7edc0f2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== DEBUGGING TRAINING ===\n",
            "\n",
            "1. Sample batch shapes:\n",
            "   input_ids: torch.Size([2, 153])\n",
            "   labels: torch.Size([2, 153])\n",
            "   attention_mask: torch.Size([2, 153])\n",
            "\n",
            "2. Labels content:\n",
            "   First 30 labels: tensor([262145, 262145,   2364,    107,   8653,    676, 236787,  12297,  13108,\n",
            "         40161,    563,    496,  57728,   2872,    528,  21593, 236789, 236751,\n",
            "         13617,   3605,   9920,  26831,    496,   7578, 236761,  12297,    563,\n",
            "           506,   6938,  36569])\n",
            "   Num valid labels (not -100): 276\n",
            "   Num -100 labels: 30\n",
            "\n",
            "3. Testing manual forward pass:\n",
            "   Manual loss: 2.9825456142425537\n",
            "\n",
            "4. Checking if LoRA params are trainable:\n",
            "   Trainable parameters: 1,474,560\n",
            "\n",
            "5. Optimizer learning rate:\n",
            "   LR: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Testing inference with trained LoRA model...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Reload tokenizer first (with fixes)\n",
        "test_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    LORA_OUTPUT_DIR,  # Load from LoRA output dir which has the saved tokenizer\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "if test_tokenizer.pad_token is None:\n",
        "    test_tokenizer.pad_token = test_tokenizer.eos_token\n",
        "\n",
        "# Reload base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    IFT_CHECKPOINT,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Load LoRA adapters\n",
        "lora_model = PeftModel.from_pretrained(base_model, LORA_OUTPUT_DIR)\n",
        "lora_model.eval()\n",
        "\n",
        "# Test prompt\n",
        "test_messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"If John has 5 apples and buys 3 more, how many apples does he have?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "test_prompt = test_tokenizer.apply_chat_template(\n",
        "    test_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "inputs = test_tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "print(f\"\\nTest Input:\\n{test_prompt}\")\n",
        "print(\"\\nGenerating response...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = lora_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=50,\n",
        "        do_sample=False,  # Changed to greedy for debugging\n",
        "        pad_token_id=test_tokenizer.pad_token_id,\n",
        "        eos_token_id=test_tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "response = test_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"\\nModel Response:\\n{response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9ZVS66HCLAc",
        "outputId": "9dba62d0-7392-4ca1-ab50-830ea4d2c617"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Testing inference with trained LoRA model...\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Input:\n",
            "<|im_start|>user\n",
            "If John has 5 apples and buys 3 more, how many apples does he have?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "\n",
            "Generating response...\n",
            "\n",
            "Model Response:\n",
            "user\n",
            "If John has 5 apples and buys 3 more, how many apples does he have?\n",
            "assistant\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Merging LoRA weights into base model...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "merged_model = lora_model.merge_and_unload()\n",
        "merged_output_path = \"/content/drive/MyDrive/gemma3_lora_merged\"\n",
        "\n",
        "merged_model.save_pretrained(merged_output_path)\n",
        "tokenizer.save_pretrained(merged_output_path)\n",
        "\n",
        "print(f\"✓ Merged model saved to: {merged_output_path}\")\n",
        "print(\"\\nAll done! 🎉\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Wuy8U7wDyPI",
        "outputId": "9e75ac61-bca3-4f21-8a5b-574fb88a0beb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking trainable parameters:\n",
            "trainable params: 1,474,560 || all params: 269,574,656 || trainable%: 0.5470\n",
            "\n",
            "Checking data collator output:\n",
            "Input IDs shape: torch.Size([2, 76])\n",
            "Labels shape: torch.Size([2, 76])\n",
            "Labels sample (first 20): tensor([262145, 262145,   2364,    107,  38447,   8150,    531,   6962,    914,\n",
            "          4916,   6077,    684,  71113,   1091, 236761,  15311,   5716,   3761,\n",
            "           795,   6360])\n",
            "Number of -100 in labels: 13\n",
            "Number of valid labels: 63\n",
            "\n",
            "Checking tokenized dataset:\n",
            "Sample input_ids: [262145, 262145, 2364, 107, 38447, 8150, 531, 6962, 914, 4916, 6077, 684, 71113, 1091, 236761, 15311, 5716, 3761, 795, 6360]\n",
            "Sample labels: [262145, 262145, 2364, 107, 38447, 8150, 531, 6962, 914, 4916, 6077, 684, 71113, 1091, 236761, 15311, 5716, 3761, 795, 6360]\n",
            "Are they the same? True\n"
          ]
        }
      ]
    }
  ]
}