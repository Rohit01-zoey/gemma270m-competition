{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOZIhOu/aZ49cxnS3DmfntQ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4A3YgsSfe9Fb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers tokenizers peft -y\n",
        "!pip install --upgrade transformers accelerate peft datasets bitsandbytes"
      ],
      "metadata": {
        "id": "h5VrUh3a-kCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import json\n",
        "\n",
        "out_path = \"/content/arc_easy_sft.jsonl\"\n",
        "\n",
        "arc_easy = load_dataset(\"allenai/ai2_arc\", \"ARC-Easy\")\n",
        "\n",
        "def choices_to_string(choices):\n",
        "    out = []\n",
        "    for c in choices[\"label\"]:\n",
        "        idx = choices[\"label\"].index(c)\n",
        "        txt = choices[\"text\"][idx]\n",
        "        out.append(f\"({c}) {txt}\")\n",
        "    return \"\\n\".join(out)\n",
        "\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as fout:\n",
        "    for item in arc_easy[\"train\"]:\n",
        "        q = item[\"question\"].strip()\n",
        "        choices = item[\"choices\"]\n",
        "        answer = item[\"answerKey\"].strip()\n",
        "\n",
        "        prompt = q + \"\\nOptions:\\n\" + choices_to_string(choices)\n",
        "\n",
        "        record = {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "                {\"role\": \"assistant\", \"content\": answer}\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        fout.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"Wrote ARC-Easy dataset to: {out_path}\")\n",
        "print(f\"Total examples: {len(arc_easy['train'])}\")\n",
        "\n",
        "SFT_DATA_PATH = \"/content/arc_easy_sft.jsonl\""
      ],
      "metadata": {
        "id": "ZGMTbA8Jpykm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import json\n",
        "\n",
        "out_path = \"/content/boolq_sft.jsonl\"\n",
        "\n",
        "boolq = load_dataset(\"google/boolq\", split=\"train\")\n",
        "boolq = boolq.shuffle(seed=42).select(range(2000))\n",
        "\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as fout:\n",
        "    for item in boolq:\n",
        "        question = item[\"question\"].strip()\n",
        "        passage = item[\"passage\"].strip()\n",
        "        answer = \"Yes\" if item[\"answer\"] else \"No\"\n",
        "\n",
        "        prompt = f\"Passage: {passage}\\n\\nQuestion: {question}\\nAnswer with Yes or No.\"\n",
        "\n",
        "        record = {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "                {\"role\": \"assistant\", \"content\": answer}\n",
        "            ]\n",
        "        }\n",
        "        fout.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "SFT_DATA_PATH = \"/content/boolq_sft.jsonl\""
      ],
      "metadata": {
        "id": "64MGSuXmSHPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IFT_CHECKPOINT = \"/content/drive/MyDrive/checkpoint-85000-darsh\"\n",
        "LORA_OUTPUT_DIR = \"/content/drive/MyDrive/gemma3_lora_sft_for_IFT_ARC-C\"\n",
        "#LORA_OUTPUT_DIR = \"/content/drive/MyDrive/gemma3_lora_boolq\"\n",
        "EPOCHS = 3"
      ],
      "metadata": {
        "id": "g0CW2N8JfIa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "import os\n",
        "import json\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "tokenizer_config_path = os.path.join(IFT_CHECKPOINT, \"tokenizer_config.json\")\n",
        "\n",
        "with open(tokenizer_config_path, 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "problematic_fields = [\"model_specific_special_tokens\", \"extra_special_tokens\"]\n",
        "for field in problematic_fields:\n",
        "    if field in config:\n",
        "        print(f\"Removing {field} field\")\n",
        "        del config[field]\n",
        "\n",
        "with open(tokenizer_config_path, 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(\"Fixed tokenizer_config.json\")\n",
        "\n",
        "import importlib\n",
        "import transformers\n",
        "if hasattr(transformers, 'tokenization_utils_base'):\n",
        "    importlib.reload(transformers.tokenization_utils_base)\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    IFT_CHECKPOINT,\n",
        "    trust_remote_code=True,\n",
        "    use_fast=True,\n",
        "    local_files_only=True\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
        "\n",
        "print(\"Loading model...\")\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    IFT_CHECKPOINT,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    IFT_CHECKPOINT,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float32,\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "id": "8NmlccyvfM9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "CZNSy1FifQMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_dataset = load_dataset(\"json\", data_files={\"train\": SFT_DATA_PATH})\n",
        "dataset = raw_dataset[\"train\"]\n",
        "\n",
        "def format_and_tokenize(example):\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        example[\"messages\"],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "\n",
        "    tokenized = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=False,\n",
        "        return_tensors=None,\n",
        "        add_special_tokens=True\n",
        "    )\n",
        "\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    format_and_tokenize,\n",
        "    remove_columns=dataset.column_names,\n",
        "    desc=\"Tokenizing dataset\"\n",
        ")\n",
        "\n",
        "print(f\"Dataset tokenized: {len(tokenized_dataset)} examples\")\n",
        "\n",
        "sample = tokenized_dataset[0]\n",
        "masked = sum(1 for x in sample['labels'] if x == -100)\n",
        "valid = sum(1 for x in sample['labels'] if x != -100)\n",
        "print(f\"  Masked tokens: {masked}\")\n",
        "print(f\"  Valid tokens: {valid}\")\n",
        "print(f\"  Masking ratio: {masked / len(sample['labels']) * 100:.1f}%\")"
      ],
      "metadata": {
        "id": "ku_bCYghfSvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List\n",
        "import torch\n",
        "\n",
        "@dataclass\n",
        "class CustomDataCollator:\n",
        "    tokenizer: AutoTokenizer\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids = [f[\"input_ids\"] for f in features]\n",
        "        labels = [f[\"labels\"] for f in features]\n",
        "        max_length = max(len(ids) for ids in input_ids)\n",
        "\n",
        "        padded_input_ids = []\n",
        "        padded_labels = []\n",
        "        attention_mask = []\n",
        "\n",
        "        for ids, lbls in zip(input_ids, labels):\n",
        "            padding_length = max_length - len(ids)\n",
        "            padded_input_ids.append(ids + [self.tokenizer.pad_token_id] * padding_length)\n",
        "            attention_mask.append([1] * len(ids) + [0] * padding_length)\n",
        "            padded_labels.append(lbls + [-100] * padding_length)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(padded_input_ids, dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
        "            \"labels\": torch.tensor(padded_labels, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "data_collator = CustomDataCollator(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "JtfN6bpOCC5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=LORA_OUTPUT_DIR,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    learning_rate=1e-4,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    fp16=False,\n",
        "    logging_steps=10,\n",
        "\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    greater_is_better=False,\n",
        "\n",
        "    report_to=\"none\",\n",
        "    optim=\"adamw_torch\",\n",
        "    warmup_steps=100,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    max_grad_norm=1.0\n",
        ")\n",
        "\n",
        "train_test_split = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = train_test_split['train']\n",
        "eval_dataset = train_test_split['test']\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "print(\"\\nStarting training\")\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(LORA_OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(LORA_OUTPUT_DIR)\n",
        "print(f\"Training complete. LoRA adapters saved to: {LORA_OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "NhG1umviXcD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Merging LoRA weights into base model...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "merged_model = lora_model.merge_and_unload()\n",
        "merged_output_path = \"/content/drive/MyDrive/gemma3_lora_merged\"\n",
        "\n",
        "merged_model.save_pretrained(merged_output_path)\n",
        "tokenizer.save_pretrained(merged_output_path)\n",
        "\n",
        "print(f\"Merged model saved to: {merged_output_path}\")"
      ],
      "metadata": {
        "id": "8Wuy8U7wDyPI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}